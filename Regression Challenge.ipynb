{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Welcome to the Regression Challenge!__\n",
    "\n",
    "This notebook provides you some code to setup a Regression problem. There is no optimal solutions, there are many good answers. We only want to see if you know how to tackle the problem. There is a lot of room for your approach to the problem. \n",
    "\n",
    "Goal: We want you to predict the number of __sales_per_day__ (= label) with the given dataset.\n",
    "\n",
    "\n",
    "Short explanation of the dataset: \n",
    "- __outlet_id__: The ID of a outlet/market\n",
    "- __country__: The country in which the outlet is located\n",
    "- __brand__: \"A\" or \"B\"\n",
    "- __customers_per_day__: The number of customers per day in this outlet \n",
    "- __sales_per_day__: The amount of sales for a specific outlet on a specific day\n",
    "- __currency__: The currency of __sales_per_day__\n",
    "- __week_id__: Calendar week \n",
    "- __weekday__: mon = Monday, tue = Tuesday, ... , sun = Sunday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn import linear_model\n",
    "import sklearn.metrics as sklm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data.csv' # maybe you have to modify this... \n",
    "data = pd.read_csv(data_path, sep=';')\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "Now it is your turn. Show us how you are tackeling this problem. You have complete freedom what you do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration and Preparation\n",
    "You have complete freedom what to do. The goal here is to explore the data with e.g. statistics and plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ... Python coding ... \n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**todo**: Columns to be converted or transformed:\n",
    "\n",
    "- to category: **outlet_id**, **brand**, **country**, **currency**, **weekday**, **week_id**\n",
    "- add level: weekday\n",
    "\n",
    "***\n",
    "\n",
    "Other features are in reasonable data types, and the label **sales_per_day** is numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data type\n",
    "def to_categories(data, cols):\n",
    "    \"\"\"\n",
    "    convert column in cols to type category\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        data[col] = data[col].astype(\"category\")\n",
    "    \n",
    "cols = [\"outlet_id\", \"brand\", \"country\", \"currency\", \"weekday\", \"week_id\"]\n",
    "to_categories(data, cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add level to weekday\n",
    "def level_categories(data, col, level):\n",
    "    \"\"\"\n",
    "    add level to col in data\n",
    "    \"\"\"\n",
    "    data[col] = data[col].astype(\"category\")\n",
    "    data[col] = data[col].cat.reorder_categories(level)\n",
    "\n",
    "\n",
    "weekday_level = [\"mon\", \"tue\", \"wed\", \"thu\", \"fri\", \"sat\", \"sun\"]\n",
    "level_categories(data, \"weekday\", weekday_level)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, the variables in the dataset are in reasonable types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"sales_per_day\"\n",
    "cat_cols = [\"brand\", \"country\", \"currency\", \"outlet_id\", \"weekday\", \"week_id\"]\n",
    "num_cols = [\"customers_per_day\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**numeric variables**: Explore the numeric variables with summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the information above, we can see that the label value, sales_per_day, there is a slight difference between the mean and the median values. The median is less than the mean, this indicates that the distribution is right-skewed, which has a tail stretching toward the right.\n",
    "\n",
    "The large amount of rows with 0 customer cannot be ignored in the dataset, it might be holidays or sundays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**categorical variable**: Explore the categorical variables with **frequency table**, in order to understand the distributions of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique(data, cat_cols):\n",
    "    for col in cat_cols:\n",
    "        print('\\n' + 'Categorical column ' + col)\n",
    "        print(data[col].value_counts())\n",
    "\n",
    "count_unique(data, cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some facts we can derive from the tables above.\n",
    "\n",
    "1. There is no imbalances in the counts of most of the categories. And for column **currency**, there is a slight difference in the counts. Since there will be few samples for category **CHF**, any statistical property for it will be poorly determined.\n",
    "\n",
    "2. For variable **outlet_id**, there are a large number of categories. For a dataset of a limited size, it is problematic, because of few samples per category.\n",
    "\n",
    "***\n",
    "These are the open points for the following modelling.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_euro(cols):\n",
    "    cur = cols[0]\n",
    "    sales = cols[1]\n",
    "    if cur == \"CHF\":\n",
    "        return sales*0.88 #change CHF to Euro\n",
    "    else:\n",
    "        return sales\n",
    "    \n",
    "data['sales_per_day'] = data[['currency','sales_per_day']].apply(change_to_euro,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Histograms: to visualize the distribution of the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kde_hist(data, cols, bins = 10, hist = False):\n",
    "    \"\"\"\n",
    "    plot kde of the columns cols of dataframe data. kde is a smoothed version of a histogram. \n",
    "    sns is used here to set the style of the grid and plot histogram.\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        sns.distplot(data[col], bins = bins, rug=False, hist = hist)\n",
    "        plt.title('Histogram of ' + col)\n",
    "        plt.xlabel(col) \n",
    "        plt.ylabel('Number of rows')\n",
    "        plt.show()\n",
    "        \n",
    "plot_kde_hist(data, num_cols, 40, hist = True)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are large amount of observations with sales value of 0. Filter them, and we get a histgram plot with clearer information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered = data[data.sales_per_day > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kde_hist(data_filtered, num_cols, 40, hist = True)  \n",
    "\n",
    "print (\"Skew is:\", data_filtered.sales_per_day.skew())\n",
    "print(\"Kurtosis: %f\" % data_filtered.sales_per_day.kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label sales_per_day has a right-skewed, not normal distribution with some multimodal tendency. The skewness will affect the statistical of machine learning  model.\n",
    "\n",
    "Let's try some feature engineering approach to make it closer to normal, so that it has better performance in modelling.\n",
    "\n",
    "**transforming numeric variables**: log transformation of sales_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered['log_sales'] = np.log(data_filtered['sales_per_day'])\n",
    "plot_kde_hist(data_filtered, ['log_sales'], 40, hist = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution above is more symmetric, but still shows some skew. However, it is still improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dive into the rows which has 0 sales_per_day, we can see that it only happens on sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_zero = data[data.sales_per_day == 0]\n",
    "count_unique(data_zero, cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scatter plots\n",
    "\n",
    "To examine the relationship between the features and the label, sales_per_day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(data, cols, col_y = label):\n",
    "    for col in cols:\n",
    "        fig = plt.figure(figsize=(7,6)) # define plot area\n",
    "        ax = fig.gca() # define axis   \n",
    "        data.plot.scatter(x = col, y = col_y, ax = ax, alpha = 0.01)\n",
    "        ax.set_title('Scatter plot of ' + col_y + ' vs. ' + col) # Give the plot a main title\n",
    "        ax.set_xlabel(col) # Set text for the x axis\n",
    "        ax.set_ylabel(col_y)# Set text for y axis\n",
    "        plt.show()\n",
    "\n",
    "cols = [\"customers_per_day\"]\n",
    "plot_scatter(data_filtered, cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = data.select_dtypes(include=[np.number])\n",
    "numeric_features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = numeric_features.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### box plot\n",
    "\n",
    "To examine the relationship between the categorical features and the label, some box plots are\n",
    "necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box(data, cols, col_y = label):\n",
    "    for col in cols:\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        sns.boxplot(col, col_y, data=data)\n",
    "        plt.xlabel(col) # Set text for the x axis\n",
    "        plt.ylabel(col_y)# Set text for y axis\n",
    "        plt.show()\n",
    "\n",
    "cols = ['brand', 'country', 'currency',  'weekday']\n",
    "plot_box(data, cols)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for some of these cases, there are some noticeable differences between the sales per day by category. For example, for weekday, there are noticeable increase by weekday and 0 on sundays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "Choose a suitable model for the Regression task or do some statistics. \n",
    "For training you can use TensorFlow, Keras, Pytorch, scikit-learn or something like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use scikit-learn package to train a AdaBoosted tree model. \n",
    "\n",
    "**reason???**\n",
    "\n",
    "**one hot encoding**: prepare dummy variables from categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def cat_encoding(data, IDcols, strCols, catCols):\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    for IDcol in IDcols:\n",
    "    #New variable id col\n",
    "        data['id_'+ IDcol] = le.fit_transform(data[IDcol])\n",
    "    new_IDcols = [('id_' + i) for i in IDcols]\n",
    "\n",
    "    for strCol in strCols:\n",
    "        if strCol in IDcols:\n",
    "            strCol = 'id_' + strCol\n",
    "        data[strCol] = le.fit_transform(data[strCol])\n",
    "\n",
    "    #Dummy Variables, one-hot encoder\n",
    "    #cat_cols = ['brand', 'country', 'outlet', 'day', 'week']\n",
    "    cat_cols = catCols + new_IDcols\n",
    "    data = pd.get_dummies(data, columns = cat_cols)\n",
    "    return data\n",
    "    #data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save prepared dataset for further reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_cols = ['outlet_id', 'week_id', 'weekday']\n",
    "cat_cols = ['brand', 'country']\n",
    "str_cols = [\"brand\", \"country\", \"weekday\"]\n",
    "data = cat_encoding(data, ID_cols, str_cols, cat_cols)\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#Drop unnecessary columns:\n",
    "data.drop(['currency'],axis=1,inplace=True)\n",
    "\n",
    "#Export f iles as modified versions:\n",
    "data.to_csv(sys.path[0]+\"/data/prepared.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared = pd.read_csv(sys.path[0]+\"/data/prepared.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = ms.train_test_split(prepared, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "def modelfit(alg, dtrain, dtest, predictors, target):\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target])\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    \n",
    "    #Remember the target had been normalized\n",
    "    #Sq_train = (dtrain[target])**2\n",
    "    #Perform cross-validation:\n",
    "    cv_score = cross_val_score(alg, dtrain[predictors], dtrain[target] , cv=20, scoring='neg_mean_squared_error')\n",
    "    cv_score = np.sqrt(np.abs(cv_score))\n",
    "    \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error((dtrain[target]).values, dtrain_predictions)))\n",
    "    print(\"CV Score : Mean - %.4g | Std - %.4g | Min - %.4g | Max - %.4g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
    "    \n",
    "    return alg\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define target and ID columns:\n",
    "target = 'sales_per_day'\n",
    "#predicted = 'predicted_sales'\n",
    "IDcols = ['week_id', 'weekday', 'outlet_id']\n",
    "predictors = train.columns.drop(['sales_per_day','week_id', 'weekday','outlet_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "LR = LinearRegression(normalize=True)\n",
    "modelfit(LR, train, test, predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "RR = Ridge(alpha=0.05,normalize=True)\n",
    "modelfit(RR, train, test, predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "DT = DecisionTreeRegressor(max_depth=15, min_samples_leaf=100)\n",
    "modelfit(DT, train, test, predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forrest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RF = RandomForestRegressor(n_estimators=10,max_depth=15, min_samples_leaf=100)\n",
    "modelfit(RF, train, test, predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "def xgboost_model(train, test, target, predictors):\n",
    "    print(\"\\nXgboost Regression\")\n",
    "    my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "    my_model.fit(train[predictors], train[target], early_stopping_rounds=5, \n",
    "                 eval_set=[(test[predictors], test[target])], verbose=False)\n",
    "\n",
    "    #Predict training set:\n",
    "    train_df_predictions = my_model.predict(train[predictors])\n",
    "\n",
    "    print(\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error((train[target]).values, train_df_predictions)))\n",
    "\n",
    "    return my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbmodel = xgboost_model(train, test, target, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among all of the regressors, Xgb performs the best with RMSE = 1332.\n",
    "\n",
    "Do model selection on filtered dataset which removed records with sales_per_day = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered = cat_encoding(data_filtered, ID_cols, str_cols, cat_cols)\n",
    "data_filtered.drop(['currency'],axis=1,inplace=True)\n",
    "train_f, test_f = ms.train_test_split(data_filtered, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_f = train_f.columns.drop(['sales_per_day','week_id', 'weekday','outlet_id'])\n",
    "\n",
    "target = 'sales_per_day'\n",
    "IDcols = ['week_id', 'weekday', 'outlet_id']\n",
    "filename = '/filtered'\n",
    "\n",
    "xgbmodel_f = xgboost_model(train_f, test_f, target, predictors_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the model selction on filtered dataset, we can see, the performance improved a lot. Therefore we can use some external public dataset such as sunday or public holiday to predict the sales for them, namely, 0 when it is sunday or public holiday. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation & Outlook\n",
    "After finishing your training take a look at sample predictions and analyse strengths and weeknesses of your model.\n",
    "\n",
    "What could be next steps to improve the model?\n",
    "\n",
    "\n",
    "- harness some public dataset of public holiday or date to predict the sales on sundays or holidays.\n",
    "- clustering the outlets and do regression on each cluster to get better performance.\n",
    "- try time series prediction techniques like vector auto regression or RNN such as LSTM on sales_per_day\n",
    "- Last but not the least, stick to simple model to have better interpretability and prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as sklm\n",
    "import math\n",
    "\n",
    "def print_metrics(y_true, y_predicted):\n",
    "    ## First compute R^2 and the adjusted R^2\n",
    "    #r2 = sklm.r2_score(y_true, y_predicted)\n",
    "    #r2_adj = r2 - (n_parameters - 1)/(y_true.shape[0] - n_parameters) * (1 - r2)\n",
    "    \n",
    "    ## Print the usual metrics and the R^2 values\n",
    "    print('Mean Square Error      = ' + str(sklm.mean_squared_error(y_true, y_predicted)))\n",
    "    print('Root Mean Square Error = ' + str(math.sqrt(sklm.mean_squared_error(y_true, y_predicted))))\n",
    "    print('Mean Absolute Error    = ' + str(sklm.mean_absolute_error(y_true, y_predicted)))\n",
    "    print('Median Absolute Error  = ' + str(sklm.median_absolute_error(y_true, y_predicted)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = xgbmodel.predict(test[predictors]) \n",
    "print_metrics(test['sales_per_day'], y_score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_f = xgbmodel_f.predict(test_f[predictors_f]) \n",
    "print_metrics(test_f['sales_per_day'], y_score_f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture \n",
    "\n",
    "Now pretend you need to build a system which runs the model/preciction functionality and should be able to make realtime predictions of the sales per day, everytime new data is generated by the source systems.  \n",
    "You need to design (a) data pipeline(s) which transfers the data from the sourcesystems to the prediction engine in the format the model/prediction engine can handle it. \n",
    "\n",
    "There are the following preconditions: \n",
    "    \n",
    "    - There are 3 source systems: \n",
    "        - System A: System which handles __sales_per_day__ \n",
    "        - System B: Delivers customers_per_day\n",
    "        - System C: Stores brand, country, currency and outled_id\n",
    "        \n",
    "    - Every system offers an API to call the information\n",
    "            \n",
    "    - You want to enable realtime predictions\n",
    "    \n",
    "    - You can use any component you like and would use for that use case. Please add to each logical component for your architecture a respective tool. (e.g. for the logical component ETL a tool named Google Dataflow)\n",
    "\n",
    "Please describe how your architecure would look like with an architecture picture. Describe how the different components will be connected and communicate. Please elaborate why you have choosen a certain logical component + tooling. We expect a moderate level of details within the architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send it to us\n",
    "In the end, please send us: \n",
    "    - The ipython notebook \n",
    "    - An detailed description of your architecture + an architecture picture (PDF). "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
